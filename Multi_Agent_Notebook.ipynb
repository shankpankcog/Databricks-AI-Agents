{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent System Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas langchain langgraph langchain_openai databricks-vectorsearch databricks-sdk openai whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from typing import TypedDict, Annotated, Sequence, Union, Dict, Any\n",
    "import operator\n",
    "import hashlib\n",
    "import uuid\n",
    "from functools import partial\n",
    "import json\n",
    "import re\n",
    "import whisper\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.vectorstores.databricks import DatabricksVectorSearch\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pyspark.sql import SparkSession\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from databricks.sdk import WorkspaceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABRICKS_HOST = \"https://your-databricks-instance.cloud.databricks.com\"\n",
    "DATABRICKS_TOKEN = \"YOUR_DATABRICKS_API_TOKEN\"\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"YOUR_VECTOR_SEARCH_ENDPOINT\"\n",
    "ENDPOINT_NAME = \"claude-3-sonnet-20240229\"\n",
    "VECTOR_SEARCH_INDEX_NAME = \"your_vector_search_index\"\n",
    "DELTA_SYNC_TABLE = \"your_delta_sync_table\"\n",
    "ERROR_LOG_TABLE = \"error_log_delta_table\"\n",
    "QUERY_CACHE_TABLE = \"query_cache_delta_table\"\n",
    "UNITY_CATALOG_NAME = \"your_unity_catalog\"\n",
    "UNITY_CATALOG_SCHEMA_NAME = \"your_schema\"\n",
    "UNSTRUCTURED_DATA_PATH = \"/dbfs/tmp/unstructured_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name: str) -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseResponse(BaseModel):\n",
    "    response_type: str\n",
    "    session_id: str\n",
    "    user_query: str\n",
    "\n",
    "class RAGResponse(BaseResponse):\n",
    "    response_type: str = \"rag\"\n",
    "    rag_content: str\n",
    "    sources: List[str]\n",
    "\n",
    "class SQLResponse(BaseResponse):\n",
    "    response_type: str = \"sql\"\n",
    "    summary: str\n",
    "    sql_query: str\n",
    "    tabular_result: str\n",
    "\n",
    "class MixedResponse(BaseResponse):\n",
    "    response_type: str = \"mixed\"\n",
    "    synthesized_answer: str\n",
    "    rag_sources: List[str]\n",
    "    sql_query: str\n",
    "\n",
    "class ErrorResponse(BaseResponse):\n",
    "    response_type: str = \"error\"\n",
    "    error_message: str\n",
    "    suggested_fix: Optional[str] = None\n",
    "\n",
    "class VoiceResponse(BaseResponse):\n",
    "    response_type: str = \"voice\"\n",
    "    summary: str\n",
    "    transcription: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_AGENT_PROMPT = \"\"\"\\nYou are an intent extraction agent. Your job is to classify the user query into a specific intent\\nand extract relevant information.\\n\\n###############################\\n# Instructions\\n###############################\\nTask:\\n  - Classify the user query into one of: Descriptive, Analytical, DrillDown, Visualization, Unstructured, Voice, Mixed.\\n  - If DrillDown, return filter_targets (values to filter the previous result on).\\n  - If Analytical, supply comparison_periods (e.g., \\\"2024 vs 2023\\\", \\\"last 6 months vs prior 6 months\\\").\\n  - Determine whether the user wants a visualization (visualization: true/false).\\n\\nOutput (STRICT JSON only):\\n{{'intent': 'Descriptive|Analytical|DrillDown|Visualization|Unstructured|Voice|Mixed', 'filter_targets': ['...'], 'comparison_periods': ['...'], 'visualization': True|False}}\\n\\nRules:\\n  - If the query mentions documents, reports, or unstructured data, use \\\"Unstructured\\\".\\n  - If the query mentions summarizing or transcribing an audio file, use \\\"Voice\\\".\\n  - If the query requires information from both documents and database tables, use \\\"Mixed\\\".\\n  - If the user asks to narrow/filter the previous output, use \\\"DrillDown\\\".\\n  - If unsure, default intent to \\\"Descriptive\\\".\\n  - Do not include explanationsâ€”return ONLY the JSON.\\n\\nFew-shot Examples:\\n1. \\\"Show New York shipments only.\\\"\\n   -> {\\\"intent\\\":\\\"DrillDown\\\",\\\"filter_targets\\\":[\\\"New York\\\"],\\\"comparison_periods\\\":[],\\\"visualization\\\":false}\\n2. \\\"Compare 2024 vs 2023 total sales.\\\"\\n   -> {\\\"intent\\\":\\\"Analytical\\\",\\\"filter_targets\\\":[],\\\"comparison_periods\\\":[\\\"2024 vs 2023\\\"],\\\"visualization\\\":false}\\n3. \\\"Summarize the project alpha review document.\\\"\\n   -> {\\\"intent\\\":\\\"Unstructured\\\",\\\"filter_targets\\\":[],\\\"comparison_periods\\\":[],\\\"visualization\\\":false}\\n4. \\\"Show trend of quantity sold over time.\\\"\\n   -> {\\\"intent\\\":\\\"Visualization\\\",\\\"filter_targets\\\":[],\\\"comparison_periods\\\":[],\\\"visualization\\\":true}\\n5. \\\"Summarize the meeting_notes.mp3 audio file.\\\"\\n    -> {\\\"intent\\\":\\\"Voice\\\",\\\"filter_targets\\\":[],\\\"comparison_periods\\\":[],\\\"visualization\\\":false}\\n\\nUser Query:\\n{user_query}\\n\"\"\"\n",
    "VOICE_SUMMARY_PROMPT = \"\"\"\\nYou are a helpful assistant. The user has provided the following transcription of an audio file.\\nPlease provide a concise summary of the text.\\n\\nTranscription:\\n{transcription}\\n\\nSummary:\\n\"\"\"\n",
    "RAG_AGENT_PROMPT = \"\"\"\\n###############################\\n# RAG Agent Instructions\\n###############################\\n\\nYour task is to answer the user's question based *only* on the provided context.\\nDo not use any prior knowledge.\\n\\nYou have been given two types of context:\\n1.  **Unstructured Context**: Excerpts from relevant documents.\\n2.  **Structured Context**: The schema of relevant database tables.\\n\\nCarefully synthesize information from both sources to provide a comprehensive answer.\\n\\n**Rules:**\\n- If the answer is found in the Unstructured Context, cite the key findings from the documents.\\n- If the answer requires information about what data is available in the database, refer to the table schemas in the Structured Context.\\n- If the user's question cannot be answered using the provided context, you MUST state that you do not have enough information to answer the question. Do not try to guess.\\n\\n###############################\\n# Provided Context\\n###############################\\n\\n**Unstructured Context (from Documents):**\\n{document_context}\\n\\n---\\n\\n**Structured Context (from Table Schemas):**\\n{schema_context}\\n\\n###############################\\n# User's Question\\n###############################\\n\\n{question}\\n\\n###############################\\n# Final Answer\\n###############################\\n\"\"\"\n",
    "TEXT_TO_SQL_AGENT_PROMPT = \"\"\"\\nYou are a SQL generation agent. Your job is to generate a single, ANSI-SQL query to satisfy the\\nuser's query given the intent and the available table schemas.\\n\\n###############################\\n# Instructions\\n###############################\\nTask:\\n  - Generate a single ANSI SQL query to satisfy the user query given the provided schema(s).\\n  - Only reference columns present in the provided schema block.\\n  - Comment each selected column with its business description (inline comments).\\n  - Apply appropriate aggregation functions (SUM, AVG, COUNT, etc.).\\n  - Handle date logic as specified.\\n\\nCRITICAL anti-hallucination rule:\\n  - Never invent columns. Use ONLY columns listed in the provided schema block.\\n  - If a requested column is not present, you MUST reply with the exact phrase:\\n      \\\"The column '<user_requested_column>' is not present in the table schema and cannot be used in the query.\\\"\\n    Output only that sentence and nothing else.\\n\\nSchema Block (you will receive text named `schema`):\\n<Formatted schema of all available tables>\\n\\nOutput rules:\\n  - Output only the SQL (no prose, no markdown).\\n\\n###############################\\n# Few-shot Examples (Generalized)\\n###############################\\n\\n-- Single-table example: Top 10 by a measure on a specific date\\n-- User Query: \\\"Top 10 ship-from accounts by total pack units on 01-08-2025\\\"\\nSELECT\\n  t1.account_identifier,                    -- links to account dimension\\n  SUM(t1.quantity_sold) AS total_quantity -- Pack Units sold\\nFROM  `{{CATALOG_NAME}}`.`{{SCHEMA_NAME}}`.`{{TABLE_1}}` t1\\nWHERE CAST(t1.transaction_date AS DATE) = '01-08-2025'\\nGROUP BY t1.account_identifier\\nORDER BY total_quantity DESC\\nLIMIT 10;\\n\\n-- Multi-table trend example: Joining two tables on a common key and time grain\\n-- User Query: \\\"Monthly trend of Net sales (from shipments) and Quantity available (from inventory) for 'Product ABC' in 2024\\\"\\nWITH data_1 AS (\\n  SELECT\\n    t1.partner_name,                                              -- trade partner\\n    DATE_TRUNC('month', CAST(t1.transaction_date AS DATE)) AS month,         -- month grain\\n    SUM(t1.net_sales) AS total_net_sales                           -- net sales\\n  FROM `{{CATALOG_NAME}}`.`{{SCHEMA_NAME}}`.`{{TABLE_1}}` AS t1\\n  WHERE t1.product_name ILIKE '%Product ABC%' AND YEAR(CAST(t1.transaction_date AS DATE)) = 2024\\n  GROUP BY t1.partner_name, DATE_TRUNC('month', CAST(t1.transaction_date AS DATE))\\n),\\ndata_2 AS (\\n  SELECT\\n    t2.partner_name,                                              -- trade partner\\n    DATE_TRUNC('month', CAST(t2.inventory_date AS DATE)) AS month,           -- month grain\\n    SUM(t2.quantity_available) AS total_quantity_available                          -- quantity available\\n  FROM `{{CATALOG_NAME}}`.`{{SCHEMA_NAME}}`.`{{TABLE_2}}` AS t2\\n  WHERE t2.product_name ILIKE '%Product ABC%' AND YEAR(CAST(t2.inventory_date AS DATE)) = 2024\\n  GROUP BY t2.partner_name, DATE_TRUNC('month', CAST(t2.inventory_date AS DATE))\\n)\\nSELECT\\n  d1.partner_name,\\n  d1.month,\\n  d1.total_net_sales,\\n  d2.total_quantity_available\\nFROM data_1 d1\\nJOIN data_2 d2\\n  ON d1.partner_name = d2.partner_name\\n AND d1.month = d2.month\\nORDER BY d1.month, d1.partner_name;\\n\\nUser Query:\\n{user_query}\\n\\nSchema:\\n{schema}\\n\\nSQL Query:\\n\"\"\"\n",
    "MIXED_INTENT_SQL_PROMPT = \"\"\"\\nGiven the user query and additional context from documents, generate a SQL query for Databricks Unity Catalog.\\n\\nAdditional Context from Documents: {rag_context}\\nUnity Catalog: {catalog}\\nSchema: {schema}\\nUser query: {user_query}\\n\\nSQL Query:\\n\"\"\"\n",
    "RESPONSE_AGENT_SQL_PROMPT = \"\"\"\\nYou are a helpful assistant. The user asked: \\\"{user_query}\\\"\\nA SQL query returned the following: {sql_result}\\nProvide a clear, business-friendly summary of the result.\\n\"\"\"\n",
    "RESPONSE_AGENT_MIXED_PROMPT = \"\"\"\\nYou have results from a document search and a database query.\\nSynthesize them to answer the user's question.\\n\\nUser Question: {user_query}\\nDocument Search Result: {rag_result}\\nDatabase Query Result: {sql_result}\\n\\nProvide a comprehensive, synthesized answer:\\n\"\"\"\n",
    "RESPONSE_AGENT_SIMPLE_PROMPT = \"\"\"\\nBased on the following information, answer the user's original question.\\n\\nUser's Question: {user_query}\\n\\nAvailable Information:\\n{engineered_context}\\n\\nFinal Answer:\\n\"\"\"\n",
    "ERROR_AGENT_PROMPT = \"\"\"\\nAn error occurred: {error}. Please suggest a fix.\\n\"\"\"\n",
    "SQL_CORRECTION_PROMPT = \"\"\"\\nThe following SQL query failed to execute. Your task is to fix it.\\n\\nUser's Original Question: \\\"{user_query}\\\"\\n\\nThe Faulty SQL Query:\\n{faulty_sql}\\n\\nThe Error Message Received:\\n{error_message}\\n\\nPlease provide only the corrected SQL query, without any additional explanation.\\nCorrected SQL Query:\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_content_safe(llm, content: str) -> bool:\n",
    "    logger.info(\"---GUARDRAIL: CHECKING CONTENT SAFETY---\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"Is the following text harmful, unethical, or inappropriate? Answer with a single word: 'yes' or 'no'. Text: \"{text}\"\"\"\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    try:\n",
    "        response = chain.invoke({\"text\": content}).strip().lower()\n",
    "        if \"yes\" in response:\n",
    "            logger.warning(\"Guardrail triggered: Potentially unsafe content detected.\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in guardrail check: {e}\")\n",
    "        return True\n",
    "\n",
    "def filter_output(llm, response: str) -> str:\n",
    "    if not is_content_safe(llm, response):\n",
    "        return \"I'm sorry, I cannot provide a response to that request.\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cache Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_cache_table(spark: SparkSession):\n",
    "    table_name = QUERY_CACHE_TABLE\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        logger.info(f\"Cache table '{table_name}' not found. Creating it.\")\n",
    "        spark.sql(f\"\"\"CREATE TABLE {table_name} (query_hash STRING, user_query STRING, final_response STRING, session_id STRING, timestamp TIMESTAMP) USING DELTA TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)\"\"\")\n",
    "    else:\n",
    "        logger.info(f\"Cache table '{table_name}' already exists.\")\n",
    "\n",
    "def get_query_hash(query: str) -> str:\n",
    "    return hashlib.md5(query.encode()).hexdigest()\n",
    "\n",
    "def check_cache(spark: SparkSession, query: str) -> str or None:\n",
    "    create_optimized_cache_table(spark)\n",
    "    query_hash = get_query_hash(query)\n",
    "    try:\n",
    "        cached_result = spark.read.table(QUERY_CACHE_TABLE).filter(col(\"query_hash\") == query_hash).select(\"final_response\").first()\n",
    "        if cached_result:\n",
    "            logger.info(f\"CACHE HIT for query: '{query}'\")\n",
    "            return cached_result['final_response']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking cache: {e}\")\n",
    "    logger.info(f\"CACHE MISS for query: '{query}'\")\n",
    "    return None\n",
    "\n",
    "def save_to_cache(spark: SparkSession, query: str, response: str, session_id: str):\n",
    "    create_optimized_cache_table(spark)\n",
    "    query_hash = get_query_hash(query)\n",
    "    cache_df = spark.createDataFrame([(query_hash, query, response, session_id)], [\"query_hash\", \"user_query\", \"final_response\", \"session_id\"])\n",
    "    cache_df = cache_df.withColumn(\"timestamp\", current_timestamp())\n",
    "    cache_df.createOrReplaceTempView(\"_new_cache_entry\")\n",
    "    merge_sql = f\"\"\"MERGE INTO {QUERY_CACHE_TABLE} t USING _new_cache_entry s ON t.query_hash = s.query_hash WHEN MATCHED THEN UPDATE SET t.final_response = s.final_response, t.session_id = s.session_id, t.timestamp = s.timestamp WHEN NOT MATCHED THEN INSERT *\"\"\"\n",
    "    logger.info(\"Saving response to cache...\")\n",
    "    spark.sql(merge_sql)\n",
    "    logger.info(\"Cache save complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Ingestion (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_id(file_path: str) -> str:\n",
    "    return hashlib.md5(file_path.encode()).hexdigest()\n",
    "\n",
    "def create_optimized_delta_table(spark: SparkSession, table_name: str, schema: StructType):\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        logger.info(f\"Table '{table_name}' does not exist. Creating it.\")\n",
    "        schema_sql = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in schema.fields])\n",
    "        spark.sql(f\"\"\"CREATE TABLE {table_name} ({schema_sql}) USING DELTA TBLPROPERTIES (delta.enableChangeDataFeed = true, delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)\"\"\")\n",
    "    else:\n",
    "        logger.info(f\"Table '{table_name}' already exists.\")\n",
    "\n",
    "def process_documents(spark: SparkSession, folder_path: str, target_table: str):\n",
    "    if not os.path.exists(folder_path):\n",
    "        logger.error(f\"The folder path '{folder_path}' does not exist.\")\n",
    "        return\n",
    "    logger.info(f\"Processing documents from: {folder_path}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    all_chunks = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.lower().endswith(\".docx\"):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            continue\n",
    "        logger.info(f\"Loading and chunking document: {filename}\")\n",
    "        documents = loader.load()\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        doc_hash = get_document_id(file_path)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"{doc_hash}_{i}\"\n",
    "            all_chunks.append((chunk_id, file_path, doc_hash, i, chunk.page_content))\n",
    "    if not all_chunks:\n",
    "        logger.info(\"No new documents to process.\")\n",
    "        return\n",
    "    schema = StructType([StructField(\"chunk_id\", StringType(), False), StructField(\"source\", StringType(), True), StructField(\"doc_hash\", StringType(), True), StructField(\"chunk_index\", IntegerType(), True), StructField(\"text_content\", StringType(), True)])\n",
    "    create_optimized_delta_table(spark, target_table, schema)\n",
    "    chunks_df = spark.createDataFrame(all_chunks, schema=schema)\n",
    "    logger.info(f\"Generated {chunks_df.count()} chunks from the documents.\")\n",
    "    chunks_df.createOrReplaceTempView(\"_new_chunks\")\n",
    "    merge_sql = f\"\"\"MERGE INTO {target_table} t USING _new_chunks s ON t.chunk_id = s.chunk_id WHEN MATCHED THEN UPDATE SET t.text_content = s.text_content, t.doc_hash = s.doc_hash WHEN NOT MATCHED THEN INSERT *\"\"\"\n",
    "    logger.info(\"Upserting chunks into Delta table with SQL MERGE...\")\n",
    "    spark.sql(merge_sql)\n",
    "    logger.info(\"Upsert complete.\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()\n",
    "if not os.path.exists(UNSTRUCTURED_DATA_PATH):\n",
    "    os.makedirs(UNSTRUCTURED_DATA_PATH)\n",
    "process_documents(spark, UNSTRUCTURED_DATA_PATH, DELTA_SYNC_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vector Search Setup (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_endpoint_to_be_ready(vsc: VectorSearchClient, endpoint_name: str):\n",
    "    for i in range(180):\n",
    "        endpoint = vsc.get_endpoint(name=endpoint_name)\n",
    "        status = endpoint.get(\"endpoint_status\", {}).get(\"state\", \"UNKNOWN\")\n",
    "        if status == \"ONLINE\":\n",
    "            logger.info(f\"Endpoint '{endpoint_name}' is online.\")\n",
    "            return\n",
    "        elif status == \"PROVISIONING\":\n",
    "            logger.info(f\"Endpoint is still provisioning... (Status: {status})\")\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            raise Exception(f\"Endpoint entered a non-recoverable state: {status}\")\n",
    "    raise Exception(f\"Endpoint '{endpoint_name}' did not become ready in time.\")\n",
    "\n",
    "def setup_vector_search_index():\n",
    "    vsc = VectorSearchClient()\n",
    "    try:\n",
    "        vsc.get_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "        logger.info(f\"Endpoint '{VECTOR_SEARCH_ENDPOINT_NAME}' already exists.\")\n",
    "    except Exception as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "            logger.info(f\"Endpoint '{VECTOR_SEARCH_ENDPOINT_NAME}' not found. Creating...\")\n",
    "            vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "            logger.info(\"Endpoint created. Waiting for it to be ready...\")\n",
    "        else:\n",
    "            raise e\n",
    "    wait_for_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "    try:\n",
    "        vsc.create_delta_sync_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=VECTOR_SEARCH_INDEX_NAME, source_table_name=DELTA_SYNC_TABLE, pipeline_type=\"CONTINUOUS\", primary_key=\"chunk_id\", embedding_source_column=\"text_content\", embedding_model_endpoint_name=\"databricks-bge-large-en\")\n",
    "        logger.info(f\"Successfully created index '{VECTOR_SEARCH_INDEX_NAME}'.\")\n",
    "    except Exception as e:\n",
    "        if \"RESOURCE_ALREADY_EXISTS\" in str(e):\n",
    "            logger.info(f\"Index '{VECTOR_SEARCH_INDEX_NAME}' already exists. Attempting to sync.\")\n",
    "            vsc.get_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=VECTOR_SEARCH_INDEX_NAME).sync()\n",
    "        else:\n",
    "            logger.error(f\"An error occurred while creating/updating the index: {e}\")\n",
    "\n",
    "setup_vector_search_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    user_query: str\n",
    "    session_id: str\n",
    "    intent: str\n",
    "    intent_details: Dict[str, Any]\n",
    "    sql_query: str\n",
    "    sql_result: str\n",
    "    rag_result: str\n",
    "    error: str\n",
    "    engineered_context: str\n",
    "    final_response: Union[BaseResponse, str]\n",
    "    from_cache: bool\n",
    "\n",
    "llm = ChatOpenAI(model=ENDPOINT_NAME, api_key=DATABRICKS_TOKEN, base_url=f\"{DATABRICKS_HOST}/serving-endpoints\")\n",
    "vsc = VectorSearchClient()\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "def get_table_schemas(spark: SparkSession, catalog: str, schema: str) -> str:\n",
    "    logger.info(f\"Fetching schemas from {catalog}.{schema}\")\n",
    "    try:\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "        schema_details = [f\"Table '{row['tableName']}': \" + \", \".join([f\"{c['col_name']} ({c['data_type']})\" for c in spark.sql(f\"DESCRIBE TABLE {catalog}.{schema}.{row['tableName']}\").collect()]) for row in tables.collect()]\n",
    "        return \"\\n\".join(schema_details)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not retrieve table schemas: {e}\")\n",
    "        return \"Could not retrieve table schemas.\"\n",
    "\n",
    "def cache_agent(state: AgentState) -> dict:\n",
    "    spark = SparkSession.builder.appName(\"CacheAgent\").getOrCreate()\n",
    "    cached_response = check_cache(spark, state['user_query'])\n",
    "    if cached_response:\n",
    "        return {\"final_response\": cached_response, \"from_cache\": True}\n",
    "    return {\"from_cache\": False}\n",
    "\n",
    "def router_agent(state: AgentState) -> dict:\n",
    "    state['messages'] = [HumanMessage(content=state['user_query'])]\n",
    "    return {\"intent\": \"clarify_intent\"}\n",
    "\n",
    "def intent_agent(state: AgentState) -> dict:\n",
    "    prompt = ChatPromptTemplate.from_template(INTENT_AGENT_PROMPT)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    try:\n",
    "        response_json = chain.invoke({\"user_query\": state['user_query']})\n",
    "        intent_details = json.loads(response_json)\n",
    "        logger.info(f\"Detected Intent Details: {intent_details}\")\n",
    "        intent = intent_details.get(\"intent\", \"Descriptive\").lower()\n",
    "        return {\"intent\": intent, \"intent_details\": intent_details}\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        logger.error(f\"Failed to parse intent JSON: {e}. Defaulting to Descriptive.\")\n",
    "        return {\"intent\": \"descriptive\", \"intent_details\": {\"intent\": \"Descriptive\"}}\n",
    "\n",
    "def text_to_sql_agent(state: AgentState) -> dict:\n",
    "    spark = SparkSession.builder.appName(\"MultiAgentSystem\").getOrCreate()\n",
    "    schema_context = get_table_schemas(spark, UNITY_CATALOG_NAME, UNITY_CATALOG_SCHEMA_NAME)\n",
    "    prompt = ChatPromptTemplate.from_template(TEXT_TO_SQL_AGENT_PROMPT)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    sql_query = chain.invoke({\"catalog\": UNITY_CATALOG_NAME, \"schema\": schema_context, \"user_query\": state['user_query']}).strip()\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            logger.info(f\"Executing SQL Query (Attempt {attempt + 1}): {sql_query}\")\n",
    "            result_df = spark.sql(sql_query)\n",
    "            result = result_df.limit(100).toPandas().to_string()\n",
    "            return {\"sql_query\": sql_query, \"sql_result\": result}\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            logger.warning(f\"SQL Query failed: {error_message}\")\n",
    "            correction_prompt = ChatPromptTemplate.from_template(SQL_CORRECTION_PROMPT)\n",
    "            correction_chain = correction_prompt | llm | StrOutputParser()\n",
    "            sql_query = correction_chain.invoke({\"user_query\": state['user_query'], \"faulty_sql\": sql_query, \"error_message\": error_message}).strip()\n",
    "    return {\"error\": f\"Failed to execute SQL query after correction: {error_message}\"}\n",
    "\n",
    "def rag_agent(state: AgentState, vsc: VectorSearchClient) -> dict:\n",
    "    try:\n",
    "        index = DatabricksVectorSearch(vsc.get_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=VECTOR_SEARCH_INDEX_NAME))\n",
    "        retriever = index.as_retriever()\n",
    "        document_context = retriever.invoke(state['user_query'])\n",
    "        spark = SparkSession.builder.appName(\"RAGAgent\").getOrCreate()\n",
    "        schema_context = get_table_schemas(spark, UNITY_CATALOG_NAME, UNITY_CATALOG_SCHEMA_NAME)\n",
    "        prompt = ChatPromptTemplate.from_template(RAG_AGENT_PROMPT)\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        result = chain.invoke({\"question\": state['user_query'], \"document_context\": document_context, \"schema_context\": schema_context})\n",
    "        return {\"rag_result\": result}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in RAG agent: {e}\")\n",
    "        return {\"error\": f\"Error in RAG agent: {e}\"}\n",
    "\n",
    "def mixed_intent_agent(state: AgentState, vsc: VectorSearchClient) -> dict:\n",
    "    rag_output = rag_agent(state, vsc)\n",
    "    if \"error\" in rag_output:\n",
    "        return {\"error\": rag_output['error']}\n",
    "    rag_context = rag_output.get(\"rag_result\", \"\")\n",
    "    spark = SparkSession.builder.appName(\"MultiAgentSystem\").getOrCreate()\n",
    "    schema_context = get_table_schemas(spark, UNITY_CATALOG_NAME, UNITY_CATALOG_SCHEMA_NAME)\n",
    "    prompt = ChatPromptTemplate.from_template(MIXED_INTENT_SQL_PROMPT)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    sql_query = chain.invoke({\"rag_context\": rag_context, \"catalog\": UNITY_CATALOG_NAME, \"schema\": schema_context, \"user_query\": state['user_query']}).strip()\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            logger.info(f\"Executing Mixed-Intent SQL Query (Attempt {attempt + 1}): {sql_query}\")\n",
    "            result_df = spark.sql(sql_query)\n",
    "            sql_result = result_df.limit(100).toPandas().to_string()\n",
    "            return {\"rag_result\": rag_context, \"sql_result\": sql_result, \"sql_query\": sql_query}\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            logger.warning(f\"Mixed-Intent SQL Query failed: {error_message}\")\n",
    "            correction_prompt = ChatPromptTemplate.from_template(SQL_CORRECTION_PROMPT)\n",
    "            correction_chain = correction_prompt | llm | StrOutputParser()\n",
    "            sql_query = correction_chain.invoke({\"user_query\": state['user_query'], \"faulty_sql\": sql_query, \"error_message\": error_message}).strip()\n",
    "    return {\"error\": f\"Failed to execute mixed-intent SQL query after correction: {error_message}\"}\n",
    "\n",
    "def voice_summarization_agent(state: AgentState) -> dict:\n",
    "    user_query = state['user_query']\n",
    "    match = re.search(r\"[\\s\\\"']?([^\\s\\\"'/]+\\.(mp3|wav|m4a|flac))[\\s\\\"']?\", user_query)\n",
    "    if not match:\n",
    "        return {\"error\": \"Could not find a valid audio file name in the query.\"}\n",
    "    audio_filename = match.group(1)\n",
    "    audio_file_path = os.path.join(UNSTRUCTURED_DATA_PATH, audio_filename)\n",
    "    if not os.path.exists(audio_file_path):\n",
    "        return {\"error\": f\"Audio file '{audio_filename}' not found in the configured data path.\"}\n",
    "    try:\n",
    "        logger.info(f\"Transcribing audio file: {audio_file_path}\")\n",
    "        result = whisper_model.transcribe(audio_file_path)\n",
    "        transcription = result[\"text\"]\n",
    "        state['messages'].append(AIMessage(content=f\"Full transcription:\\n{transcription}\"))\n",
    "        logger.info(\"Transcription complete. Generating summary...\")\n",
    "        prompt = ChatPromptTemplate.from_template(VOICE_SUMMARY_PROMPT)\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        summary = chain.invoke({\"transcription\": transcription})\n",
    "        response_model = VoiceResponse(session_id=state['session_id'], user_query=user_query, summary=summary, transcription=transcription)\n",
    "        return {\"final_response\": response_model}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during voice processing: {e}\")\n",
    "        return {\"error\": f\"Error during voice processing: {e}\"}\n",
    "\n",
    "def context_engineer_agent(state: AgentState) -> dict:\n",
    "    context_parts = []\n",
    "    if state.get(\"messages\"):\n",
    "        history = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in state[\"messages\"]])\n",
    "        context_parts.append(f\"Conversation History:\\n{history}\")\n",
    "    if state.get(\"rag_result\"):\n",
    "        context_parts.append(f\"Information from Documents:\\n{state['rag_result']}\")\n",
    "    if state.get(\"sql_result\"):\n",
    "        context_parts.append(f\"Database Query Results:\\n{state['sql_result']}\")\n",
    "    if not context_parts:\n",
    "        return {\"error\": \"No context was generated by the previous steps.\"}\n",
    "    engineered_context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    logger.info(f\"Engineered Context:\\n{engineered_context}\")\n",
    "    return {\"engineered_context\": engineered_context}\n",
    "\n",
    "def response_agent(state: AgentState) -> dict:\n",
    "    if isinstance(state.get(\"final_response\"), BaseResponse):\n",
    "        return {\"final_response\": state[\"final_response\"].model_dump_json()}\n",
    "    engineered_context = state.get(\"engineered_context\")\n",
    "    if not engineered_context:\n",
    "        return llm_error_agent({\"error\": \"Context engineering failed.\"})\n",
    "    prompt = ChatPromptTemplate.from_template(RESPONSE_AGENT_SIMPLE_PROMPT)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    final_answer = chain.invoke({\"user_query\": state['user_query'], \"engineered_context\": engineered_context})\n",
    "    response_model = RAGResponse(session_id=state['session_id'], user_query=state['user_query'], rag_content=final_answer, sources=[\"Synthesized from multiple sources\"])\n",
    "    return {\"final_response\": response_model.model_dump_json()}\n",
    "\n",
    "def create_optimized_error_log_table(spark: SparkSession):\n",
    "    table_name = ERROR_LOG_TABLE\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        logger.info(f\"Error log table '{table_name}' not found. Creating it.\")\n",
    "        spark.sql(f\"\"\"CREATE TABLE {table_name} (timestamp TIMESTAMP, session_id STRING, user_query STRING, error_message STRING) USING DELTA TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true)\"\"\")\n",
    "\n",
    "def llm_error_agent(state: AgentState) -> dict:\n",
    "    error_message = state.get(\"error\", \"An unknown error occurred.\")\n",
    "    spark = SparkSession.builder.appName(\"ErrorLogging\").getOrCreate()\n",
    "    create_optimized_error_log_table(spark)\n",
    "    try:\n",
    "        error_data = [(state.get(\"session_id\"), state.get(\"user_query\"), error_message)]\n",
    "        error_df = spark.createDataFrame(error_data, [\"session_id\", \"user_query\", \"error_message\"])\n",
    "        error_df = error_df.withColumn(\"timestamp\", current_timestamp())\n",
    "        error_df.write.format(\"delta\").mode(\"append\").saveAsTable(ERROR_LOG_TABLE)\n",
    "        logger.info(f\"Error successfully logged to {ERROR_LOG_TABLE}\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to log error to Delta table: {e}\")\n",
    "    prompt = ChatPromptTemplate.from_template(ERROR_AGENT_PROMPT)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    suggestion = chain.invoke({\"error\": error_message})\n",
    "    error_model = ErrorResponse(session_id=state.get(\"session_id\"), user_query=state.get(\"user_query\"), error_message=error_message, suggested_fix=suggestion)\n",
    "    return {\"final_response\": error_model.model_dump_json()}\n",
    "\n",
    "def save_cache_agent(state: AgentState) -> dict:\n",
    "    spark = SparkSession.builder.appName(\"SaveCacheAgent\").getOrCreate()\n",
    "    response_to_cache = state['final_response'] if isinstance(state['final_response'], str) else state['final_response']\n",
    "    save_to_cache(spark, state['user_query'], response_to_cache, state['session_id'])\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "rag_agent_with_vsc = partial(rag_agent, vsc=vsc)\n",
    "mixed_intent_agent_with_vsc = partial(mixed_intent_agent, vsc=vsc)\n",
    "workflow.add_node(\"cache_agent\", cache_agent)\n",
    "workflow.add_node(\"router\", router_agent)\n",
    "workflow.add_node(\"intent_agent\", intent_agent)\n",
    "workflow.add_node(\"rag_agent\", rag_agent_with_vsc)\n",
    "workflow.add_node(\"text_to_sql_agent\", text_to_sql_agent)\n",
    "workflow.add_node(\"mixed_intent_agent\", mixed_intent_agent_with_vsc)\n",
    "workflow.add_node(\"voice_summarization_agent\", voice_summarization_agent)\n",
    "workflow.add_node(\"context_engineer_agent\", context_engineer_agent)\n",
    "workflow.add_node(\"response_agent\", response_agent)\n",
    "workflow.add_node(\"llm_error_agent\", llm_error_agent)\n",
    "workflow.add_node(\"save_cache\", save_cache_agent)\n",
    "workflow.set_entry_point(\"cache_agent\")\n",
    "workflow.add_conditional_edges(\"cache_agent\", lambda state: \"continue\" if not state.get(\"from_cache\") else \"end\", {\"continue\": \"router\", \"end\": END})\n",
    "workflow.add_edge(\"router\", \"intent_agent\")\n",
    "workflow.add_conditional_edges(\"intent_agent\", lambda state: state[\"intent\"], {\"descriptive\": \"text_to_sql_agent\", \"analytical\": \"text_to_sql_agent\", \"drilldown\": \"text_to_sql_agent\", \"visualization\": \"text_to_sql_agent\", \"unstructured\": \"rag_agent\", \"mixed\": \"mixed_intent_agent\", \"voice\": \"voice_summarization_agent\"})\n",
    "workflow.add_conditional_edges(\"rag_agent\", lambda state: \"error\" if state.get(\"error\") else \"continue\", {\"continue\": \"context_engineer_agent\", \"error\": \"llm_error_agent\"})\n",
    "workflow.add_conditional_edges(\"text_to_sql_agent\", lambda state: \"error\" if state.get(\"error\") else \"continue\", {\"continue\": \"context_engineer_agent\", \"error\": \"llm_error_agent\"})\n",
    "workflow.add_conditional_edges(\"mixed_intent_agent\", lambda state: \"error\" if state.get(\"error\") else \"continue\", {\"continue\": \"context_engineer_agent\", \"error\": \"llm_error_agent\"})\n",
    "workflow.add_conditional_edges(\"voice_summarization_agent\", lambda state: \"error\" if state.get(\"error\") else \"continue\", {\"continue\": \"context_engineer_agent\", \"error\": \"llm_error_agent\"})\n",
    "workflow.add_edge(\"context_engineer_agent\", \"response_agent\")\n",
    "workflow.add_edge(\"response_agent\", \"save_cache\")\n",
    "workflow.add_edge(\"llm_error_agent\", \"save_cache\")\n",
    "workflow.add_edge(\"save_cache\", END)\n",
    "langgraph_app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"YOUR_QUERY_HERE\"\n",
    "session_id = str(uuid.uuid4())\n",
    "inputs = {\"user_query\": user_query, \"session_id\": session_id}\n",
    "for output in langgraph_app.stream(inputs, {\"recursion_limit\": 10}):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
